{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45398736-7e89-4263-89c8-92153baff553",
   "metadata": {},
   "source": [
    "**LLM Workshop 2024 by Sebastian Raschka**\n",
    "\n",
    "This code is based on *Build a Large Language Model (From Scratch)*, [https://github.com/rasbt/LLMs-from-scratch](https://github.com/rasbt/LLMs-from-scratch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66dd524e-864c-4012-b0a2-ccfc56e80024",
   "metadata": {
    "id": "66dd524e-864c-4012-b0a2-ccfc56e80024"
   },
   "source": [
    "# 4) Pretraining LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80c5537d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.10.3-cp312-cp312-macosx_11_0_arm64.whl.metadata (11 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Downloading contourpy-1.3.2-cp312-cp312-macosx_11_0_arm64.whl.metadata (5.5 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Downloading fonttools-4.58.0-cp312-cp312-macosx_10_13_universal2.whl.metadata (104 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Downloading kiwisolver-1.4.8-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: numpy>=1.23 in /Users/jefferycao/miniconda3/envs/llm/lib/python3.12/site-packages (from matplotlib) (2.1.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/jefferycao/miniconda3/envs/llm/lib/python3.12/site-packages (from matplotlib) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in /Users/jefferycao/miniconda3/envs/llm/lib/python3.12/site-packages (from matplotlib) (11.2.1)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib)\n",
      "  Downloading pyparsing-3.2.3-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/jefferycao/miniconda3/envs/llm/lib/python3.12/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/jefferycao/miniconda3/envs/llm/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Downloading matplotlib-3.10.3-cp312-cp312-macosx_11_0_arm64.whl (8.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.1/8.1 MB\u001b[0m \u001b[31m37.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading contourpy-1.3.2-cp312-cp312-macosx_11_0_arm64.whl (255 kB)\n",
      "Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.58.0-cp312-cp312-macosx_10_13_universal2.whl (2.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m35.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading kiwisolver-1.4.8-cp312-cp312-macosx_11_0_arm64.whl (65 kB)\n",
      "Downloading pyparsing-3.2.3-py3-none-any.whl (111 kB)\n",
      "Installing collected packages: pyparsing, kiwisolver, fonttools, cycler, contourpy, matplotlib\n",
      "Successfully installed contourpy-1.3.2 cycler-0.12.1 fonttools-4.58.0 kiwisolver-1.4.8 matplotlib-3.10.3 pyparsing-3.2.3\n"
     ]
    }
   ],
   "source": [
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92b989e9-da36-4159-b212-799184764dd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matplotlib version: 3.10.3\n",
      "numpy version: 2.1.1\n",
      "tiktoken version: 0.7.0\n",
      "torch version: 2.7.0\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "\n",
    "pkgs = [\"matplotlib\", \n",
    "        \"numpy\", \n",
    "        \"tiktoken\", \n",
    "        \"torch\",\n",
    "       ]\n",
    "for p in pkgs:\n",
    "    print(f\"{p} version: {version(p)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3bdf9e-2ff0-4a57-abab-ede2d955a237",
   "metadata": {},
   "source": [
    "- In this notebook, we implement the training loop and code for basic model evaluation to pretrain an LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd27fcc-2886-47cb-b544-046c2c31f02a",
   "metadata": {},
   "source": [
    "<img src=\"figures/01.png\" width=1000px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc1cf3f-82d8-46c7-9ecc-58979ce87cdd",
   "metadata": {
    "id": "bdc1cf3f-82d8-46c7-9ecc-58979ce87cdd"
   },
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "\n",
    "\n",
    "# 4.1 Using GPT to generate text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3415fd-9f4a-4548-908e-9dfa56edc9bc",
   "metadata": {},
   "source": [
    "- We initialize a GPT model using the code from the previous notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86000d74-624a-48f0-86da-f41926cb9e04",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "86000d74-624a-48f0-86da-f41926cb9e04",
    "outputId": "ad482cfd-5a62-4f0d-e1e0-008d6457f512"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from supplementary import GPTModel\n",
    "\n",
    "\n",
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,   # Vocabulary size\n",
    "    \"context_length\": 256, # Shortened context length (orig: 1024)\n",
    "    \"emb_dim\": 768,        # Embedding dimension\n",
    "    \"n_heads\": 12,         # Number of attention heads\n",
    "    \"n_layers\": 12,        # Number of layers\n",
    "    \"drop_rate\": 0.1,      # Dropout rate\n",
    "    \"qkv_bias\": False      # Query-key-value bias\n",
    "}\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.eval();  # Disable dropout during inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c6cf0f-7458-48a2-97fd-aa5068d65e8c",
   "metadata": {},
   "source": [
    "- We use dropout of 0.1 above, but it's relatively common to train LLMs without dropout nowadays\n",
    "- Modern LLMs also don't use bias vectors in the `nn.Linear` layers for the query, key, and value matrices (unlike earlier GPT models), which is achieved by setting `\"qkv_bias\": False`\n",
    "- We reduce the context length (`context_length`) of only 256 tokens to reduce the computational resource requirements for training the model, whereas the original 124 million parameter GPT-2 model used 1024 tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f80895-be35-4bb5-81cb-f357ef7367fe",
   "metadata": {},
   "source": [
    "- Next, we use the `generate_text_simple` function from the previous chapter to generate text\n",
    "- In addition, we define two convenience functions, `text_to_token_ids` and `token_ids_to_text`, for converting between token and text representations that we use throughout this chapter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741881f3-cee0-49ad-b11d-b9df3b3ac234",
   "metadata": {},
   "source": [
    "<img src=\"figures/02.png\" width=1200px>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e062b82-3540-48ce-8eb4-009686d0d16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "from supplementary import generate_text_simple\n",
    "\n",
    "\n",
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0) # add batch dimension\n",
    "    return encoded_tensor\n",
    "\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    flat = token_ids.squeeze(0) # remove batch dimension\n",
    "    return tokenizer.decode(flat.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6516f757-849c-468f-88f7-28ac9debf6be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you rentingetic wasnم refres RexMeCHicular stren\n"
     ]
    }
   ],
   "source": [
    "start_context = \"Every effort moves you\"\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(start_context, tokenizer),\n",
    "    max_new_tokens=10,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d3249b-b2a0-44c4-b589-ae4b403b8305",
   "metadata": {},
   "source": [
    "- As we can see above, the model does not produce good text because it has not been trained yet\n",
    "- How do we measure or capture what \"good text\" is, in a numeric form, to track it during training?\n",
    "- The next subsection introduces metrics to calculate a loss metric for the generated outputs that we can use to measure the training progress\n",
    "- The next chapters on finetuning LLMs will also introduce additional ways to measure model quality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955f9e1a-7bf7-40d8-b1fa-eacabdee8d8e",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec6c217-e429-40c7-ad71-5d0a9da8e487",
   "metadata": {
    "id": "2ec6c217-e429-40c7-ad71-5d0a9da8e487"
   },
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "\n",
    "\n",
    "# 4.2 Preparing the dataset loaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530da89e-2448-436c-8f1b-28e8a31ef85c",
   "metadata": {},
   "source": [
    "- We use a relatively small dataset for training the LLM (in fact, only one short story)\n",
    "  - The training finishes relatively fast (minutes instead of weeks), which is good for educational purposes\n",
    "- For example, Llama 2 7B required 184,320 GPU hours on A100 GPUs to be trained on 2 trillion tokens\n",
    " \n",
    "- Below, we use the same dataset we used in the data preparation notebook earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "654fde37-b2a9-4a20-a8d3-0206c056e2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    text_data = file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379330f1-80f4-4e34-8724-41d892b04cee",
   "metadata": {},
   "source": [
    "- A quick check that the text loaded ok by printing the first and last 100 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6kgJbe4ehI4q",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "6kgJbe4ehI4q",
    "outputId": "9ff31e88-ee37-47e9-ee64-da6eb552f46f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
     ]
    }
   ],
   "source": [
    "# First 100 characters\n",
    "print(text_data[:99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "j2XPde_ThM_e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "j2XPde_ThM_e",
    "outputId": "a900c1b9-9a87-4078-968b-a5721deda5cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it for me! The Strouds stand alone, and happen once--but there's no exterminating our kind of art.\"\n"
     ]
    }
   ],
   "source": [
    "# Last 100 characters\n",
    "print(text_data[-99:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6b46a952-d50a-4837-af09-4095698f7fd1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6b46a952-d50a-4837-af09-4095698f7fd1",
    "outputId": "c2a25334-21ca-486e-8226-0296e5fc6486"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Characters: 20479\n",
      "Tokens: 5145\n"
     ]
    }
   ],
   "source": [
    "total_characters = len(text_data)\n",
    "total_tokens = len(tokenizer.encode(text_data))\n",
    "\n",
    "print(\"Characters:\", total_characters)\n",
    "print(\"Tokens:\", total_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8830cb9-90f6-4e7c-8620-beeabc2d39f7",
   "metadata": {},
   "source": [
    "- With 5,145 tokens, the text is very short for training an LLM, but again, it's for educational purposes (we will also load pretrained weights later)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bedcad87-a0e8-4b9d-ac43-4e927ccbb50f",
   "metadata": {},
   "source": [
    "- Next, we divide the dataset into a training and a validation set and use the data loaders from chapter 2 to prepare the batches for LLM training\n",
    "- For visualization purposes, the figure below assumes a `max_length=6`, but for the training loader, we set the `max_length` equal to the context length that the LLM supports\n",
    "- The figure below only shows the input tokens for simplicity\n",
    "    - Since we train the LLM to predict the next word in the text, the targets look the same as these inputs, except that the targets are shifted by one position"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46bdaa07-ba96-4ac1-9d71-b3cc153910d9",
   "metadata": {},
   "source": [
    "<img src=\"figures/03.png\" width=1500px>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0959c855-f860-4358-8b98-bc654f047578",
   "metadata": {},
   "outputs": [],
   "source": [
    "from supplementary import create_dataloader_v1\n",
    "\n",
    "\n",
    "# Train/validation ratio\n",
    "train_ratio = 0.90\n",
    "split_idx = int(train_ratio * len(text_data))\n",
    "train_data = text_data[:split_idx]\n",
    "val_data = text_data[split_idx:]\n",
    "\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_loader = create_dataloader_v1(\n",
    "    train_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=True,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "val_loader = create_dataloader_v1(\n",
    "    val_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=False,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ac3296-a4d1-4303-9ac5-376518960c33",
   "metadata": {},
   "source": [
    "- We use a relatively small batch size to reduce the computational resource demand, and because the dataset is very small to begin with\n",
    "- Llama 2 7B was trained with a batch size of 1024, for example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e0514d-b990-4dc0-9afb-7721993284a0",
   "metadata": {},
   "source": [
    "- An optional check that the data was loaded correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ca0116d0-d229-472c-9fbf-ebc229331c3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader:\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "\n",
      "Validation loader:\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n"
     ]
    }
   ],
   "source": [
    "print(\"Train loader:\")\n",
    "for x, y in train_loader:\n",
    "    print(x.shape, y.shape)\n",
    "\n",
    "print(\"\\nValidation loader:\")\n",
    "for x, y in val_loader:\n",
    "    print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b9b1a4-863d-456f-a8dd-c07fb5c024ed",
   "metadata": {},
   "source": [
    "- Another optional check that the token sizes are in the expected ballpark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eb860488-5453-41d7-9870-23b723f742a0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eb860488-5453-41d7-9870-23b723f742a0",
    "outputId": "96b9451a-9557-4126-d1c8-51610a1995ab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training tokens: 4608\n",
      "Validation tokens: 512\n",
      "All tokens: 5120\n"
     ]
    }
   ],
   "source": [
    "train_tokens = 0\n",
    "for input_batch, target_batch in train_loader:\n",
    "    train_tokens += input_batch.numel()\n",
    "\n",
    "val_tokens = 0\n",
    "for input_batch, target_batch in val_loader:\n",
    "    val_tokens += input_batch.numel()\n",
    "\n",
    "print(\"Training tokens:\", train_tokens)\n",
    "print(\"Validation tokens:\", val_tokens)\n",
    "print(\"All tokens:\", train_tokens + val_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3085e8-665e-48eb-bb41-cdde61537e06",
   "metadata": {},
   "source": [
    "- Next, let's calculate the initial loss before we start training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0691332-84d0-48b3-b462-a885ddeb4fca",
   "metadata": {},
   "source": [
    "- If you have a machine with a CUDA-supported GPU, the LLM will train on the GPU without making any changes to the code\n",
    "- Via the `device` setting, we ensure that the data is loaded onto the same device as the LLM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "56f5b0c9-1065-4d67-98b9-010e42fc1e2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 10.98758347829183\n",
      "Validation loss: 10.98110580444336\n"
     ]
    }
   ],
   "source": [
    "from supplementary import calc_loss_loader\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device) # no assignment model = model.to(device) necessary for nn.Module classes\n",
    "\n",
    "torch.manual_seed(123) # For reproducibility due to the shuffling in the data loader\n",
    "\n",
    "with torch.no_grad(): # Disable gradient tracking for efficiency because we are not training, yet\n",
    "    train_loss = calc_loss_loader(train_loader, model, device)\n",
    "    val_loss = calc_loss_loader(val_loader, model, device)\n",
    "\n",
    "print(\"Training loss:\", train_loss)\n",
    "print(\"Validation loss:\", val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9339f8d-00cb-4206-af67-58c32bd72055",
   "metadata": {
    "id": "b9339f8d-00cb-4206-af67-58c32bd72055"
   },
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "\n",
    "\n",
    "# 4.3 Training an LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652a4cf4-e98f-46d9-bdec-60e7ccb8d6bd",
   "metadata": {},
   "source": [
    "- In this section, we finally implement the code for training the LLM\n",
    "\n",
    "<img src=\"figures/04.png\" width=700px>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Mtp4gY0ZO-qq",
   "metadata": {
    "id": "Mtp4gY0ZO-qq"
   },
   "outputs": [],
   "source": [
    "from supplementary import (\n",
    "    calc_loss_batch,\n",
    "    evaluate_model,\n",
    "    generate_and_print_sample\n",
    ")\n",
    "\n",
    "def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n",
    "                       eval_freq, eval_iter, start_context, tokenizer):\n",
    "    # Initialize lists to track losses and tokens seen\n",
    "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
    "    tokens_seen, global_step = 0, -1\n",
    "\n",
    "    # Main training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set model to training mode\n",
    "        \n",
    "        for input_batch, target_batch in train_loader:\n",
    "            optimizer.zero_grad() # Reset loss gradients from previous batch iteration\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            loss.backward() # Calculate loss gradients\n",
    "            optimizer.step() # Update model weights using loss gradients\n",
    "            tokens_seen += input_batch.numel()\n",
    "            global_step += 1\n",
    "\n",
    "            # Optional evaluation step\n",
    "            if global_step % eval_freq == 0:\n",
    "                train_loss, val_loss = evaluate_model(\n",
    "                    model, train_loader, val_loader, device, eval_iter)\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                track_tokens_seen.append(tokens_seen)\n",
    "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
    "                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
    "\n",
    "        # Print a sample text after each epoch\n",
    "        generate_and_print_sample(\n",
    "            model, tokenizer, device, start_context\n",
    "        )\n",
    "\n",
    "    return train_losses, val_losses, track_tokens_seen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a301b333-b9d4-4eeb-a212-3a9874e3ac47",
   "metadata": {},
   "source": [
    "- Now, let's train the LLM using the training function defined above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3422000b-7aa2-485b-92df-99372cd22311",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3422000b-7aa2-485b-92df-99372cd22311",
    "outputId": "0e046603-908d-4093-8ae5-ef2f632639fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000000): Train loss 9.781, Val loss 9.933\n",
      "Ep 1 (Step 000005): Train loss 8.111, Val loss 8.339\n",
      "Every effort moves you,,,,,,,,,,,,.                                     \n",
      "Ep 2 (Step 000010): Train loss 6.661, Val loss 7.048\n",
      "Ep 2 (Step 000015): Train loss 5.961, Val loss 6.616\n",
      "Every effort moves you, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and,, and, and,\n",
      "Ep 3 (Step 000020): Train loss 5.726, Val loss 6.600\n",
      "Ep 3 (Step 000025): Train loss 5.201, Val loss 6.348\n",
      "Every effort moves you, and I had been.                                            \n",
      "Ep 4 (Step 000030): Train loss 4.417, Val loss 6.278\n",
      "Ep 4 (Step 000035): Train loss 4.069, Val loss 6.226\n",
      "Every effort moves you know the                          \"I he had the donkey and I had the and I had the donkey and down the room, I had\n",
      "Ep 5 (Step 000040): Train loss 3.732, Val loss 6.160\n",
      "Every effort moves you know it was not that the picture--I had the fact by the last I had been--his, and in the            \"Oh, and he said, and down the room, and in\n",
      "Ep 6 (Step 000045): Train loss 2.850, Val loss 6.179\n",
      "Ep 6 (Step 000050): Train loss 2.427, Val loss 6.141\n",
      "Every effort moves you know,\" was one of the picture. The--I had a little of a little: \"Yes, and in fact, and in the picture was, and I had been at my elbow and as his pictures, and down the room, I had\n",
      "Ep 7 (Step 000055): Train loss 2.104, Val loss 6.134\n",
      "Ep 7 (Step 000060): Train loss 1.882, Val loss 6.233\n",
      "Every effort moves you know,\" was one of the picture for nothing--I told Mrs.  \"I was no--as! The women had been, in the moment--as Jack himself, as once one had been the donkey, and were, and in his\n",
      "Ep 8 (Step 000065): Train loss 1.320, Val loss 6.238\n",
      "Ep 8 (Step 000070): Train loss 0.985, Val loss 6.242\n",
      "Every effort moves you know,\" was one of the axioms he had been the tips of a self-confident moustache, I felt to see a smile behind his close grayish beard--as if he had the donkey. \"strongest,\" as his\n",
      "Ep 9 (Step 000075): Train loss 0.717, Val loss 6.293\n",
      "Ep 9 (Step 000080): Train loss 0.541, Val loss 6.393\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back the window-curtains, I had the donkey. \"There were days when I\n",
      "Ep 10 (Step 000085): Train loss 0.391, Val loss 6.452\n",
      "Every effort moves you know,\" was one of the axioms he laid down across the Sevres and silver of an exquisitely appointed luncheon-table, when, on a later day, I had again run over from Monte Carlo; and Mrs. Gis\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)\n",
    "\n",
    "num_epochs = 10\n",
    "train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "    model, train_loader, val_loader, optimizer, device,\n",
    "    num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n",
    "    start_context=\"Every effort moves you\", tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "139885c4-40ed-4765-b307-511d5a967fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0WSRu2i0iHJE",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "id": "0WSRu2i0iHJE",
    "outputId": "9d36c61b-517d-4f07-a7e8-4563aff78b11"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from supplementary import plot_losses\n",
    "\n",
    "\n",
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc83ded-5f80-4e1c-bf4d-ccb59999d995",
   "metadata": {},
   "source": [
    "- Looking at the results above, we can see that the model starts out generating incomprehensible strings of words, whereas towards the end, it's able to produce grammatically more or less correct sentences\n",
    "- However, based on the training and validation set losses, we can see that the model starts overfitting\n",
    "- If we were to check a few passages it writes towards the end, we would find that they are contained in the training set verbatim -- it simply memorizes the training data\n",
    "\n",
    "- There are decoding strategies (not covered in this workshop) that can mitigate this memorization by a certain degree\n",
    "- Also note that the overfitting here occurs because we have a very, very small training set, and we iterate over it so many times\n",
    "  - The LLM training here primarily serves educational purposes; we mainly want to see that the model can learn to produce coherent text\n",
    "  - Instead of spending weeks or months on training this model on vast amounts of expensive hardware, we load pretrained weights later"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58ebc3a-34d1-4efe-94a0-ef5bec732162",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "\n",
    "\n",
    "# Exercise 1: Generate text from the pretrained LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25558c3-a4f4-48de-a18e-ed63ff9ee02a",
   "metadata": {},
   "source": [
    "- Use the model to generate new text (HINT: scroll up to see how we generated text before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1dac44fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "Hello, what's your name? on--forming, as it were, so inevitably the background of the house.\"\n",
      "\n",
      "\"I didn't about her poverty. Gisburn's open countenance. \"It's his ridiculous modesty, you know. He says they're not fit\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(device)\n",
    "start_context = \"Hello, what's your name?\"\n",
    "context_size = model.pos_emb.weight.shape[0]\n",
    "encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
    "with torch.no_grad():\n",
    "    token_ids = generate_text_simple(\n",
    "        model=model, idx=encoded,\n",
    "        max_new_tokens=50, context_size=context_size\n",
    "    )\n",
    "    decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d62ff8c-78ea-47fa-b02d-9313531cb4df",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "\n",
    "\n",
    "# Exercise 2: Load the pretrained model in a new session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a62addc-41ed-4853-8aec-365ef4611f79",
   "metadata": {},
   "source": [
    "- Open a new Python session or Jupyter notebook and load the model there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f596c405",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import tiktoken\n",
    "from supplementary import GPTModel\n",
    "\n",
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,   # Vocabulary size\n",
    "    \"context_length\": 256, # Shortened context length (orig: 1024)\n",
    "    \"emb_dim\": 768,        # Embedding dimension\n",
    "    \"n_heads\": 12,         # Number of attention heads\n",
    "    \"n_layers\": 12,        # Number of layers\n",
    "    \"drop_rate\": 0.1,      # Dropout rate\n",
    "    \"qkv_bias\": False      # Query-key-value bias\n",
    "}\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "model.load_state_dict(torch.load(\"model.pth\", map_location=torch.device(\"cpu\"), weights_only=True))\n",
    "model.eval();  # Disable dropout during inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bb74f21d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ello the prism of Hermia's tears I felt able to face the fact with equanimity. Poor Jack Gisburn! The women had made him--it was fitting that they should mourn him. Among his own sex fewer regrets were heard, and\n"
     ]
    }
   ],
   "source": [
    "generate_and_print_sample(model=model, tokenizer=tokenizer, device=device, start_context=\"Ello\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4b25e3-d1aa-4559-897c-36588bba2057",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "\n",
    "# Exercise 3 (Optional): Train the LLM on your own favorite texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ded438bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"LESSWRONG\n",
    "Book Review: On the Edge: The Fundamentals\n",
    "23rd Sep 2024\n",
    "Don't Worry About the Vase\n",
    "The most likely person to write On the Edge was Nate Silver.\n",
    "\n",
    "Grok thinks the next most likely was Michael Lewis, followed by a number of other writers of popular books regarding people thinking different.\n",
    "\n",
    "I see why Grok would say that, but it is wrong.\n",
    "\n",
    "The next most likely person was Zvi Mowshowitz.\n",
    "\n",
    "I haven’t written a book for this type of audience, a kind of smarter business-book, but that seems eminently within my potential range.\n",
    "\n",
    "On the Edge is a book about those living On The Edge, the collection of people who take risk and think probabilistically and about expected value. It centrally covers poker, sports betting, casinos, Silicon Valley, venture capital, Sam Bankman-Fried, effective altruism, AI and existential risk.\n",
    "\n",
    "Collectively, Nate Silver calls this cultural orientation The River.\n",
    "\n",
    "It is contrasted with The Village, which comprises roughly the mainstream mostly left-of-center institutions, individuals and groups that claim that they are The Experts and the Very Serious People.\n",
    "\n",
    "If you are thinking about Secret Third Thing, that Village plus River very much does not equal America, I was thinking a lot about that too. Hold that thought.\n",
    "\n",
    "The book is a collection of different topics. So this review is that, as well.\n",
    "\n",
    "The central theme here will be Yes, And.\n",
    "\n",
    "Nate Silver wrote On the Edge for people who are not in The River. I suspect his main target was The Village, but there are also all the people who are neither. He knows a lot more than he is saying here, but it is a popular book, and popular books have to start at the beginning. There was a lot to cover.\n",
    "\n",
    "This joy of this review? I don’t have to do any of that. This is aimed at those who read things I write. Which means most of you already know a lot, often most, of what is in at least large portions of On the Edge.\n",
    "\n",
    "So this is a chance to go deeper, be more detailed and opinionated, with a different world model in many ways, and expertise in different spots along the River.\n",
    "\n",
    "As with my other book reviews, quotes by default are from the book, and the numbers in parenthesis are book locations on Kindle. I sometimes insert additional paragraph breaks, and I fix capitalization after truncating quotes while being careful to preserve original intent. Also notice that I change around the order when it improves flow.\n",
    "\n",
    "This review is in four parts, which I plan to post throughout the week: The Fundamentals (this post), The Gamblers, The Business and The Future.\n",
    "\n",
    "\n",
    "Overview\n",
    "On the Edge is a series of stories tied to together by The River and risk taking.\n",
    "\n",
    "I see this as a book in four parts, which I’ve rearranged a bit for my review.\n",
    "\n",
    "This post will cover The Fundamentals: The introduction, the overall concepts of The River and the Village, and various universal questions about risk taking. That includes the book’s introduction, and universal discussions pulled from later on.\n",
    "\n",
    "Part 1 of the book, which I will cover in the second post The Gamblers, is about the world of gambling. You have poker, sports betting and casinos.\n",
    "\n",
    "This was my favorite part of the book.\n",
    "\n",
    "I have some experience with these topics, and got what I would call Reverse Gell-Mann Amnesia.\n",
    "\n",
    "In normal Gell-Mann Amnesia, you notice the newspaper gets wrong the things you know the most about. Then you go on to not assume that the newspaper is equally inaccurate on other topics.\n",
    "\n",
    "In reverse Gell-Mann Amnesia, you notice that the book is getting the details right. Not even once, while covering these topics, did I think to myself, ‘oh, that’s wrong, Nate got fooled or confused here.’\n",
    "\n",
    "Are there places where I would have emphasized different points, taken a different perspective or added more information, or even disagreed? Oh, sure. But I’ve read a lot of this style of book, and Nate Silver definitely Gets It. This is as good as this format allows.\n",
    "\n",
    "Drayton’s review found Nate making a number of ‘elementary’ mistakes in other areas. And yes, once you get out of Nate’s wheelhouse, there are some errors. But they’re not central or importantly conceptual, as far as I could tell.\n",
    "\n",
    "Part 2 of the book, which I will cover in what I’ll call The Business, was about gamblers who play for higher stakes over longer periods, betting on real world things. As in, we talk about stock traders, Silicon Valley and venture capital. I think that the spirit is largely on point, but that on many details Nate Silver here buys a bit too much into the insider story that gets pitched to him and other outsiders, in ways that assume the virtues of good gamblers and The River are present to a greater extent than they are. They’re there, but not as much as we’d like.\n",
    "\n",
    "Part 3 of the book, which I will also include in The Business, was about crypto and especially Sam Bankman-Fried. This part was a let down, and I’m mostly going to skip over it. On basic crypto my readers know all this already.\n",
    "\n",
    "After Going Infinite and my review of that, it did not feel like there was much to add on SBF. I worry this tied presented SBF as more central and important to The River and especially to EA and similar areas than he actually was. I do get why Nate Silver felt he had to cover this, and why he had to run with it once he had it. We’ll hit some highlights that are relatively unique, but mostly gloss over it.\n",
    "\n",
    "Part 4 of the book, which I will cover in The Future, was about AI and existential risk, including rationalists and EAs. He picks excellent sources: Sam Altman, Roon, Ajeya, Scott Alexander, Oliver Habryka, Eliezer Yudkowsky. He also talked to me, although I did not end up being quoted.\n",
    "\n",
    "The parts that discuss the history of OpenAI reflect Silver essentially buying Altman’s party line in ways I found disappointing. I will do my best to point to my corrections of the record and distinctions in perspective.\n",
    "\n",
    "The parts that talk about AI technically will be nothing new to blog readers here. I don’t think he got anything wrong, but we will mostly skip this.\n",
    "\n",
    "Then there is the discussion of AI existential risk, and the role of the EA and rationalist communities. While I was disappointed, especially after the excellent start, I totally see how Nate got where he ended up on all this. It was a real outsider attempt to look at the situation, and here I can bring superior knowledge and arguments to bear.\n",
    "\n",
    "Nate’s overall view, that existential risk is obviously real and important if you think AI is going to keep advancing, but that we cannot at this time afford to simply choose not to proceed, seems incomplete but eminently reasonable. Long book is long, and necessary background information is a big problem, but the discussion of existential risk arguments felt extremely abrupt and cut off, in ways the rest of the book did not. In contrast, he spends a bunch of time arguing we should worry about technological stagnation if we do not proceed with AI.\n",
    "\n",
    "One thing about the final section I loved was the Technological Richter Scale. This was very good Rhetorical Innovation, asking people to place AI on a logarithmic scale of impact compared to other technologies. This reveals better than other methods that many, perhaps most, disagreements about AI existential risk are actually disagreements about AI capabilities – those not worried about AI largely do not believe AI will be ‘all that.’ I covered the scale in its own post, so it will have its own reference point.\n",
    "\n",
    "Introduction: The River\n",
    "What is The River?\n",
    "\n",
    "Every book like this needs a fake framework, a new set of categories to tie together chapters about various topics, that then they see everywhere.\n",
    "\n",
    "What is The River?\n",
    "\n",
    "The River is a sprawling ecosystem of like-minded people that includes everyone from low-stakes poker pros just trying to grind out a living to crypto kings and venture-capital billionaires. It is a way of thinking and a mode of life. People don’t know very much about the River, but they should.\n",
    "\n",
    "Most Riverians aren’t rich and powerful. But rich and powerful people are disproportionately likely to be Riverians compared to the rest of the population. (73)\n",
    "\n",
    "The River Nature is a way of thinking and a mode of life.\n",
    "\n",
    "If you have that nature by default, you are a Riverian, a citizen of the The River.\n",
    "\n",
    "If your group or activity rewards and celebrates that nature, then it is along The River.\n",
    "\n",
    "In this mode, you think in terms of probabilities and expected value (EV). You seek the most accurate possible model of the world, including what actions are how likely to lead to what results. You the riverian look to make the best decisions possible. You are not afraid of risk, but seek to take only the good risks that are +EV.\n",
    "\n",
    "You can then be somewhat risk averse when making decisions, risk neutral or even risk loving. All riverians have weaknesses, ways they systematically mess up. The key is, you accept that risk is part of life, and you look to make the most of it, including understanding that sometimes the greatest risk is not taking one.\n",
    "\n",
    "Riverians are the Advantage Players of life.\n",
    "\n",
    "They want life to be about everyone making good decisions. A True Riverian learns to inherently love a correct play and hate a mistake, in all contexts, from all sides that are not their active opponents. They want those good decisions and valuable actions to be rewarded, the bad decisions and destructive actions punished. They want that to be what matters, not who you know or who you are or how you play some political game.\n",
    "\n",
    "Riverians hate being told what to do if they don’t think it will help them win. They despise when others boss them around and tell them to do dumb things, or are told to copy what others around them do without justification.\n",
    "\n",
    "The River is where people focus on being right, taking chances and doing what works, and not letting anyone tell them different.\n",
    "\n",
    "As you would expect, The River and its inhabitants often looks stupid. Things are reliably blowing up in various faces and others, especially The Village, are often quick to highlight such failures.\n",
    "\n",
    "Given everything that took place while I was writing this book—poker cheating scandals; Elon Musk’s transformation from rocket-launching renegade into X edgelord; the spectacular self-induced implosion of Sam Bankman-Fried—you’d think the River had a rough few years. But guess what: the River is winning. (77)\n",
    "\n",
    "Few would accuse Elon Musk or SBF of being the innocent victims of bad luck regarding recent events in their lives. Mistakes, as they say, were made. Massive, historical mistakes. But the alternative, the world and culture and nature where such mistakes are not happening, where people don’t take successful risks and then get into position to take even bigger ones, is worse, not better.\n",
    "\n",
    "As a fellow inhabitant of The River, this one rings far more true and important than most. I am definitely convinced the River is real, and that it definitely includes most of the groups listed in the book, although we’ll see there is one case I think is less clear.\n",
    "\n",
    "One very clear truth is that playing poker or betting on sports is Not So Different from investing in tech startups or investing (sometimes ‘investing’) in crypto tokens.\n",
    "\n",
    "The River isn’t all fun and games. The activities that everyone agrees are capital-G Gambling—like blackjack and slots and horse racing and lotteries and poker and sports betting—are really just the tip of the iceberg. They are fundamentally not that different from trading stock options or crypto tokens, or investing in new tech startups. (94)\n",
    "\n",
    "If you had to divide that collection into two groups, you could divide it into the casino gambling on one side and the ‘investments’ on the other, and that would be valid.\n",
    "\n",
    "Almost as valid would be to move poker and sports betting and other skill games into the ‘investment’ category, and leave slot machines and craps and other non-skill games in the other (with the exception of a small number of Advantage Players, which the book explores). In practice, a zero-day stock option is closer to a sports bet than it is to buying the stock and holding it for a month. More on that throughout.\n",
    "\n",
    "There is quite a lot of that actual straight up gambling.\n",
    "\n",
    "Literal gambling is booming. In 2022, Americans lost around $60 billion betting at licensed casinos and online gambling operations—a record even after accounting for inflation. They also lost an estimated $40 billion in unlicensed, gray-market, or black-market gambling—and about $30 billion in state lotteries. To be clear, that’s the amount they lost, not the amount they wagered, which was roughly ten times as much. (188)\n",
    "\n",
    "A total of $130 billion means the average adult lost on the order of $500 gambling, but of course that is wildly unevenly distributed. Most lost nothing or very little. A few lost a lot.\n",
    "\n",
    "The multiplier depends on the game. A 10x multiplier for casinos and grey market gambling seems reasonable.\n",
    "\n",
    "For state lotteries, the multiplier is… less.\n",
    "\n",
    "On average, the government keeps about 35 cents of every dollar you spend on a lottery ticket, and some states keep 80 percent or more. Lottery tickets are purchased disproportionately by the poor. (2871)\n",
    "\n",
    "As the game Illuminati describes the state lottery, it’s a tax on stupidity, and the money rolls in. That is unfair. Slightly. Only slightly. It is absurd how terrible the official lotteries are.\n",
    "\n",
    "Nate Silver Comes Home to The River\n",
    "There is nothing like being where you belong to remind you who you are, as Nate experienced after going to his first real poker tournament after Covid.\n",
    "\n",
    "The other big realization I had on that flight home from Florida was that this world of poker players and poker-playing types—this world of calculated risk-taking—was the world where I fit in. (206)\n",
    "\n",
    "And yet, the people in the River are my tribe—and I wouldn’t have it any other way. Why did my conversations flow so naturally with people in the River, even when they were on subjects I was still learning more about? (378)\n",
    "\n",
    "Why indeed? Why does he think he fits in so well?\n",
    "\n",
    "First, there’s what I call the “cognitive cluster.” Quite literally: How do people in the River think about the world? It begins with abstract and analytical reasoning. (387)\n",
    "\n",
    "The natural companion to analytic thinking is abstract thinking—that is, trying to derive general rules or principles from the things you observe in the world. Another way to describe this is “model building.” (391)\n",
    "\n",
    "Then there’s the “personality cluster.” These traits are more self-explanatory. People in the River are trying to beat the market. (422)\n",
    "\n",
    "Relatedly, people in the River are often intensely competitive. (430)\n",
    "\n",
    "Finally, I put risk tolerance in this cluster because—whether they’re degens or nits in other parts of their lives—being willing to break from the herd and go against the consensus is certainly not the safest professional path. (436)\n",
    "\n",
    "Nate’s history is that he was a poker player, happily minding his own business, then Congress sneaked a provision into a bill that killed American online poker and took away his job.\n",
    "\n",
    "There was one silver lining: the UIGEA piqued my interest in politics. The bill had been tucked into an unrelated piece of homeland security legislation and passed during the last session before Congress recessed for the midterms. It was a shifty workaround, and having essentially lost my job, I wanted the people responsible for it to lose their jobs, too. (235)\n",
    "\n",
    "I have a deeply similar story, with sports betting and the Safe Port Act. Congress tucks a provision into a different law and suddenly online sports betting transforms and being a sports better in America became vastly more difficult. Both of us got fired.\n",
    "\n",
    "Nate went into politics. I chose a different angle of response. We both did well in our new modeling work, and then both got frustrated over time.\n",
    "\n",
    "In Nate’s case, the problem was that election forecasts and regular people don’t mix.\n",
    "\n",
    "But here’s the thing about having tens of millions of people viewing your forecast: a lot of them aren’t going to get it. (246)\n",
    "\n",
    "Expected value is such a foundational concept in the River’s way of thinking that 2016 served as a litmus test for who in my life was a member of the tribe and who wasn’t. At the same moment a certain type of person was liable to get very mad at me, others were thrilled that they’d been able to use FiveThirtyEight’s forecast to make a winning bet. (267)\n",
    "\n",
    "But permit me this one-time informal use of “rational”: people are really fucking irrational about elections (308)\n",
    "\n",
    "Likewise, the tendency in the media is to contextualize ideas—The New York Times is no longer just the facts, but a “juicy collection of great narratives,” as Ben Smith described it. (420)\n",
    "\n",
    "In my case, those I worked with declared (and this is a direct quote) ‘the age of heroes is over,’ cut back on risk and investment accordingly, and I wept that this meant there were no more worlds to conquer. So I left for others.\n",
    "\n",
    "We are both classic cases of learning probability for gambling reasons, then eventually applying it to places that matter. It is most definitely The Way.\n",
    "\n",
    "Blaise Pascal and Pierre de Fermat developed probability theory in response to a friend’s inquiry about the best strategy in a dice game. (367)\n",
    "\n",
    "Nate (In General) Makes One Critical Mistake\n",
    "He notices, but he can’t stop himself.\n",
    "\n",
    "I feel like it’s my sacred duty to call out someone who’s wrong on the internet. (6417)\n",
    "\n",
    "I indeed see him doing this a lot, especially on Twitter. Nate Silver, with notably rare exceptions, you do not have to do this. Let it go.\n",
    "\n",
    "The Village Idiots\n",
    "There’s also another community that competes with the River for power and influence. I call it the Village. (440)\n",
    "\n",
    "The Village are the Respectable Authority Figures. The Very Serious People.\n",
    "\n",
    "It consists of people who work in government, in much of the media, and in parts of academia (although perhaps excluding some of the more quantitative academic fields such as economics). It has distinctly left-of-center politics associated with the Democratic Party. (442)\n",
    "\n",
    "My title for this section is not entirely fair to The Village. It is also not as unfair as it sounds. Members of The Village are usually above average in intelligence and skill and productivity. The vast majority of people are not in either Village or River.\n",
    "\n",
    "But yeah, in the ways Village and River differ strongly? I mostly stand by it. The failure to use the River Nature, the contrast in modes of cognition, is stupefying.\n",
    "\n",
    "In some contexts, those not in The Village proper will attempt to play the role of The Village in a given context. In doing so, they take on The Village Nature, to attempt to operate from that same aura of authority and expertise. And yes, it reliably makes them act and talk stupider.\n",
    "\n",
    "What makes people far stupider than that is being trapped in the Hegelian dialectic, and in particular the one of party politics.\n",
    "\n",
    "Indeed, Riverians inherently distrust political parties, particularly in a two-party system like the United States where they are “big tent” coalitions that couple together positions on dozens of largely unrelated issues. Riverians think that partisan position taking often serves as a shortcut for the more nuanced and rigorous analysis that public intellectuals ought to engage in. (466)\n",
    "\n",
    "That is Nate Silver bending over backwards to be polite. Ask most members of The River, whether they back one party, the other or neither, and they will say something similar that is… less polite.\n",
    "\n",
    "The Village also believes that Riverians are naïve about how politics works and about what is happening in the United States. Most pointedly, it sees Donald Trump and the Republican Party as having characteristics of a fascist movement and argues that it is time for moral clarity and unity against these forces. (510)\n",
    "\n",
    "The Village thinks that if you do not give up your epistemics to support their side of the Hegelian dialectic, then you lack moral clarity and are naive. No, that claim did not start with Donald Trump. Nor is it confined to The River.\n",
    "\n",
    "Riverians are fierce advocates for free speech, not just as a constitutional right but as a cultural norm. (488)\n",
    "\n",
    "To the extent they express an opinion on the issue, whether or not they belong to The River, every single person whose opinion I respect is a strong advocate for free speech.\n",
    "\n",
    "I remember when I thought The Village believed in free speech too. No longer. Some members of The Village do. Overall it does not.\n",
    "\n",
    "That is a huge problem. The Village that exists today is very different from The Village that my parents thought they were members of back in the day. I would still ultimately have the River Nature, but I miss the old Village.\n",
    "\n",
    "Alone in the Wilderness\n",
    "I buy that there exist The River and The Village.\n",
    "\n",
    "What about everyone and everything else?\n",
    "\n",
    "This becomes most obvious when the book or author discusses Donald Trump.\n",
    "\n",
    "Obviously Donald Trump is not of The Village.\n",
    "\n",
    "The temptation is to place him in The River, but that is also obviously wrong.\n",
    "\n",
    "Donald Trump may have an appetite for some forms of risk and even for casinos, but he does not have The River Nature. He does not think in probabilities and expected values. He might want to run a casino, but that is because he was in the real estate business, not because he has any affinity for River-style gamblers.\n",
    "\n",
    "If you look at Donald Trump’s supporters, it becomes even clearer. These people hate The Village, but most also view The River as alien. They don’t think in probability any more than Villagers do. When either group goes to a casino, almost none of them are looking for advantage bets. They, like most people and perhaps more so, are deeply suspicious of markets, and those who speak in numbers and abstractions.\n",
    "\n",
    "The Village might be in somewhat of a cold war with The River, but the River is not its natural enemy or mirror. Something else is that.\n",
    "\n",
    "So what do we call this third group? Not ‘everyone not in the Village or River’ and not ‘the other political party’ but rather: The natural enemies of The Village?\n",
    "\n",
    "I asked for the LLM consensus is in, and there is a clear winner that I agree is indeed this group’s True Name in this schema, that works on many levels: The Wilderness.\n",
    "\n",
    "In the extended metaphor, it used to be that Village and River were natural allies. Now that this is not the case. The Village presents the world as a Hegelian dialectic between it and the Wilderness, treating every other group including the River as irrelevant or some side show.\n",
    "\n",
    "Their constant message to the River is: You don’t f***ing matter. Their other message is that they do not tolerate neutrality. When the Village turns on you and yours for not falling in line – and the River Nature as a matter of principle does not bow down, which is a big hint as to who they centrally are – but especially when the Village turns directly on you, you feel cast out and targeted.\n",
    "\n",
    "Thus, increasingly, some members of The River, and others who The Village casts out over some Shibboleth, end up in The Wilderness. This is a deeply tragic process, as they abandon The River Nature and embrace The Wilderness Nature, inevitably embracing an entire basket of positions, usually well past the point of sanity.\n",
    "\n",
    "See: Elon Musk.\n",
    "\n",
    "Does that still leave a Secret Fourth Thing?\n",
    "\n",
    "One can of course keep going.\n",
    "\n",
    "Most people, even if they ‘put their trust in’ the Village, Wilderness or River, or even The Vortex. They do not at core have any of these natures. They are good people trying to go about their business in peace. One can call this The People.\n",
    "\n",
    "(Note: I tried to make this fit the Magic color wheel in a fun way, but it didn’t work.)\n",
    "\n",
    "An alternative telling here in another good book review suggests The Fort as the right-wing mirror image of The Village. The Fort is where Ted Cruz and Samuel Alito hang out. It’s important to note that The Wilderness and The Fort are not the same place. And we both agree that The People are a distinct other thing.\n",
    "\n",
    "Why the River Hates the Village\n",
    "The actual section is actually “Why the Valley Hates the Village (4756)” but this is one case where I think one can push the book thesis further. Yes, centrally Silicon Valley, but the entire River hates the Village, mostly for the same underlying reasons.\n",
    "\n",
    "Those reasons, centrally, are in my own words something like this:\n",
    "\n",
    "The Village in many ways does mean well.\n",
    "\n",
    "But it fundamentally views the world as a morality play and Hegelian dialectic. The us against the them. ‘Good causes’ to advance against enemies like ignorance and greed. Those ‘good causes’ and their Shibboleths are often chosen based on what feels good to endorse on a surface level, rather than what would actually do good. Often they get hijacked by various social dynamics spun out of control, often caused by people who do not mean well. They rarely ask deeply about the effectiveness of their proposals.\n",
    "\n",
    "Not only do they think this is going on, they think it is the only important thing going on. And they think primarily on Simulacra level 3, in terms of alliances and implications and how things sound about saying what type of person you are and are allied with, rather than about what actually causes what.\n",
    "\n",
    "When they lie or break the rules or the norms of common decency it is for a good cause. When others do it they cry bloody murder. And they justify all that by pointing at The Wilderness and saying, have you seen The Other Guys? As if there were only two options.\n",
    "\n",
    "They have never understood economics and incentives and value creation, or trade-offs, treating you as a bad person if you point out or care about such considerations. They treat your success and your wealth and legacy as fundamentally not yours, and think they have a right to take it from you, and that not doing so would be unfair. They have great respect for certain particular local details that made their Shibboleth and Good Cause lists, while completely ignoring vital others and Seeing Like a State.\n",
    "\n",
    "Indeed, if you have other priorities than theirs, if you break even one of their Shibboleths or triggers, or fail to sufficiently support the wrong one at the wrong time, they often cast you out into The Wilderness, doing their best to have people place you there regardless of whether that makes any sense. And the resulting costs, in the form of the inability to Do Things of various sorts, is growing over time, to the point where our civilization is in rather deep trouble.\n",
    "\n",
    "To be fair, trust in Big Tech has also dropped sharply in polls. But Silicon Valley is not particularly dependent on public confidence so long as it continues to recruit talent and people continue to buy its products. (5268)\n",
    "\n",
    "Big Tech now sees the specter of actively dangerous enemy action. So does what Marc Andreessen calls ‘Little Tech.’ So does the rest of The River.\n",
    "\n",
    "The River, in the past, mostly put up with The Village. The Village has a lot of practical advantages, did broadly mean well, and there were common enemies who were seen as clearly worse. And importantly, The Village was mostly leaving The River alone in kind, or at least giving it some space in which to safely operate, and the paralysis effects were nowhere near as bad.\n",
    "\n",
    "Starting around 2016, a lot of that changed, and other parts reached tipping points. Also they blew their remaining credibility and legitimacy in increasingly stupid ways, cumulating in various events around the time of Covid. The Village’s case for why The River should accept its authoriah is down to a mix of ‘we have the legible expert labels and are Very Serious People’ and ‘you should see The Other Guy (e.g. The Wilderness’). Both are increasingly failing to hold water.\n",
    "\n",
    "The first because lol. The second because at some point that’s a risk people in The River will be willing to take. Also decision theory says you can’t let them play you like that indefinitely. We can’t let the Hegelian dialectic win.\n",
    "\n",
    "There was a deal, well beyond how media coverage works. The Village broke the pact.\n",
    "\n",
    "This deal’s getting worse and worse all the time. The rent got too damn high.\n",
    "\n",
    "Nate Silver’s History of River Versus Village\n",
    "Everyone got really mad at each other about 2016. (4762)\n",
    "\n",
    "That was a big breaking point. Meta is awful, Facebook is awful, Mark Zuckerberg is awful, and also their AI positions might well doom us all. But the story that a few Facebook ads were why Trump beat Hillary Clinton in 2016 was always absurd.\n",
    "\n",
    "And yet, that became The Narrative, in many circles. That had big effects.\n",
    "\n",
    "One that Nate doesn’t focus on is Gell-Mann Amnesia. If The Experts are so convinced of something like this, that is clear Obvious Nonsense, what else that they tell you is Obvious Nonsense?\n",
    "\n",
    "Another is that Big Tech, and tech in general, got a clear lesson in the Copenhagen Interpretation of Ethics. If tech is seen interacting with something, they were informed, then tech will be blamed for the result. Which, given how much tech interacts with everything, is a real problem.\n",
    "\n",
    "Patrick McKenzie has noted that one big result of this was that when we got a Covid vaccine, we had a big problem. Rather than reach out for Google’s help, the government tried to muddle through without it. Google and Apple and Amazon did not dare step forward and offer help in getting shots into arms, for fear of blowback. Even if they got it right, they feared they would be showing up the government.\n",
    "\n",
    "That is how a handful of volunteers on a Discord server, VaccniateCA, ended up becoming our source of information on the vaccine.\n",
    "\n",
    "The Village is about group allegiance, while Silicon Valley is individualistic. (4782)\n",
    "\n",
    "The Village really is about group allegiance, and also increasingly (at least until about 2020) group identity. Which groups and ideas you are for, which ones you are against. They are centrally Simulacra Level 3, although they also spend time at 1, 2 and 4.\n",
    "\n",
    "The River is individualistic, as Nate says, but even more it is about facts and outcomes and ground truth: Simulacra Level 1. Most River factions are very Level 1 focused.\n",
    "\n",
    "Silicon Valley is the major part of the River that compromises on this the most. They talk about the importance of Level 1, ‘build something people want.’ Yet they are very willing to care deeply about The Vibes, and often primarily operate more on Level 4 (and at times the Level 4 message is exactly that you too should be doing this), as well as some of Levels 2 and 3. A certain amount of lying, or at least selective presentation, is expected, as is loyalty to the group and its concepts. It’s complicated.\n",
    "\n",
    "Meritocracy is another big deal for The River. Back in the day they could tell the story that The Village was that way too, but that story got harder to tell over time.\n",
    "\n",
    "Especially in explicit politics, it was clear that merit was going unrewarded.\n",
    "\n",
    "But campaigns are not always very meritocratic. “It’s very rare to actually be able to assess whether someone did a good job,” Shor said. Campaigns have hundreds of staffers and ultimately only one real test—election night—of how well they did, which is often determined by circumstances outside the campaign’s control. Relationships matter more than merit. So people get ahead by going with the program. (4790)\n",
    "\n",
    "Extreme merit still gets rewarded, if you have a generational political talent. In the absence of that, the variance overwhelms skill, so politics rules politics.\n",
    "\n",
    "There are turf wars—and philosophical ones—between Silicon Valley and Washington over regulation. (4801)\n",
    "\n",
    "This is natural and expected. To some extent everyone is fine with it. The problem is that there are signs this may be taken way too far, in ways that kill or severely damage the Valley’s business model. See the proposals for unrealized taxes on capital gains, for various crazy things that almost get done with social media, and now various claims about AI.\n",
    "\n",
    "Silicon Valley is skeptical of the “trust the experts” mantra that the Village prizes. (4832)\n",
    "\n",
    "Skeptical is a nice word for how The River views this by default, in worlds where the experts are plausibly trustworthy. Then various things happened, and kept happening. Increasingly, ‘trust the experts’ became an argument from authority, and from status within the Village, and its mask as a ‘scientific consensus’ resulting from truth seeking became that much harder to maintain.\n",
    "\n",
    "Tech leaders are in an ideological clash with their employees and blame the Village for it. (4855)\n",
    "\n",
    "Yes. Yes, they do. It is not a reasonable way to look at the situation, which involved things such as:\n",
    "\n",
    "In Silicon Valley, you’re supposed to feel like you have permission to express unpopular and possibly quite wrong or even stupid ideas. So Damore’s firing represented a shift. (4878)\n",
    "\n",
    "The Village created a social climate, especially among tech employees, where Google felt forced to fire Damore and engage in other similar actions, as did many other tech companies. Based on what I have heard, in many places including Google things got very out of hand. This did not sit well.\n",
    "\n",
    "But what about Silicon Valley’s elites—the top one hundred VCs, CEOs, and founders? There’s no comprehensive catalog of their political views, so I’ll just give you my impressions as a reporter who’s had a variety of conversations with them.\n",
    "\n",
    "It’s worth keeping in mind that rich people are usually conservative. If Silicon Valley’s elites were voting purely with their pocketbooks, they’d vote Republican for lower taxes and fewer regulations, especially with Khan heading the FTC. (4862)\n",
    "\n",
    "The elites for a while faced sufficient pressure from the Village, and especially from their employees, they did not dare move against the Village and felt like they were being forced to abandon many core River values and to support democrats despite the financial problems with that and the constant attempts by the Village to attack the SV elites, lower their status and potentially confiscate their wealth and break up their businesses and plans. Recently that danger and fear of ideological backlash has subsided, everyone is sufficiently fed up and worried about actual legal consequences, and feels like the masks are already off, and so there is a lot more open hostility.\n",
    "\n",
    "Another classic clash point was Thiel getting his revenge on Denton for outing Thiel.\n",
    "\n",
    "I asked Thiel about this passage. Hadn’t he been a hypocrite to focus on destroying a rival, Denton, half a continent away in New York in a completely unrelated business, all for the sin of outing a gay man who lived in the world’s gayest city, San Francisco?\n",
    "\n",
    "Thiel quickly conceded the point. “In any intensely competitive context, it is almost impossible to simply focus on a transcendent object and not spend a lot of time on the personalities of one’s rivals.” (4900)\n",
    "\n",
    "I think it is highly rational and reasonable decision theory to retaliate for that, even if you think everything turned out fine for Thiel after being outed. If someone hits you like that, they are doing it in part because they don’t think you’ll hit back, and others are watching and asking the same question. It is +EV to make everyone think twice, and especially to in advance be the type of person who would do that, especially in a way others can notice. Which Thiel very much was, but Denton didn’t pay enough attention, or didn’t care, so he found out.\n",
    "\n",
    "One thing that did not ring true for me at all was this:\n",
    "\n",
    "I asked Swisher why tech leaders like Thiel and Musk are so obsessed with their media coverage. She didn’t need much time to consider her answer. “It’s because they’re narcissists. They’re all malignant narcissists,” she said. (4908)\n",
    "\n",
    "I’ve long had to mute Kara Swisher. She is constantly name calling, launching unfounded personal attacks and carrying various people’s water, and my emotional state reliably got worse every time I saw anything she wrote without providing me with useful information. She is not a good information source, and this type of comment is exactly why.\n",
    "\n",
    "Yes, they care about their image, but their image is vital to their business and life strategy. I never got this sense from Thiel at all, and I don’t have any info you don’t have about Musk but certainly he cares far more than most people about big abstract important things, even when I think he’s playing badly.\n",
    "\n",
    "Spending Time at Airports\n",
    "A toy risk question that came up later, to get us started.\n",
    "\n",
    "SBF then made an analogy that you’d think would trigger my sympathies—but actually raised my alarm. “If you’re making a decision, such as there’s no way that it goes really badly, then I sort of feel like—you know, zero is not the correct number of times to miss a flight. If you never miss a flight, you’re spending too much time in airports.”\n",
    "\n",
    "I used to think about air travel like this. I even went through a phase where I took a perverse joy in trying to arrive as close to the departure time as possible and still make the flight.\n",
    "\n",
    "Now that I’m more mature and have a credit card that gives me access to the Delta Sky Club, I don’t cut it quite so close. But the reason you should be willing to risk missing a flight is because the consequences are usually quite tolerable. (6033)\n",
    "\n",
    "A fun tip I recently realized is that if you never miss a flight, that generally means you are spending too little time at airports.\n",
    "\n",
    "Explanation: If you actual never miss a flight that means you don’t fly enough, since no amount of sane buffer gets your risk anywhere near zero, airlines be tripping.\n",
    "\n",
    "On the practical question, the key indeed is that there is limited upside and limited downside. This can and should be a calculated risk. Missing your flight is sometimes super expensive, sometimes trivially cheap – there are times you’ll take a $500 buyout to skip the flight happily (although note to airlines: You’ll get a much better price off me if you ask me before I head to the airport!), others where even for thousands the answer is a very clear no.\n",
    "\n",
    "And there are times when leaving an extra two hours at the airport is a trivial cost, you weren’t doing anything vital and have plenty of good podcasts and books. Other times, every minute before you leave is valuable. And also there are considerations like lounge access.\n",
    "\n",
    "I don’t have lounge access, but I’ve found that spending time at airports is mostly remarkably pleasant. You have power, you have a phone and if you want a laptop and tablet, there’s shops and people watching, it’s kind of a mini-vacation before the flight if you have a good attitude. If you have an even better attitude, so is the flight.\n",
    "\n",
    "You would not, however, say ‘if you are still alive you are not doing enough skydiving.’\n",
    "\n",
    "(Or that you are not building sufficiently advanced AIs, but I digress.)\n",
    "\n",
    "The Coin Flip\n",
    "If your decision is very close, why not randomize? Nate does this often, to his partner’s frequent dismay.\n",
    "\n",
    "We’re indifferent between the Italian place and the Indian place, there’s no reason to waste our time agonizing over the decision. (1049)\n",
    "\n",
    "I agree I should do this more. Instead, I try to use semi-random determinations, with the same ‘if I made a big mistake I will notice and fix it, and if I made a small mistake who cares’ rule attached.\n",
    "\n",
    "However I also do frequently spend more time on close decisions. I think this can be good praxis. It is wasteful in the moment, but going into detail on close decisions is a great way to learn how to make better decisions. So in any decision where it would be great to improve your algorithm, if it is very close, you might want to overthink things for that reason.\n",
    "\n",
    "At other times, yeah, it doesn’t matter. Flip that coin.\n",
    "\n",
    "The Other Risk Takers\n",
    "The thesis is that wherever you find highly successful risk takers, you see the same patterns, the same River Nature.\n",
    "\n",
    "[Those are] hardly the only people who undertake risks. So I want to introduce you to five exceptional people who take physical risks: an astronaut, an athlete, an explorer, a lieutenant general, and an inventor. (3925)\n",
    "\n",
    "The thesis: If you want to succeed at risk taking, you need to be Crazy Prepared. You need to take calculated risks with your eyes open. You need the drive to succeed enough to justify the risks. You need all the good things.\n",
    "\n",
    "Even if they aren’t quantitative per se, they are highly rigorous thinkers, meticulous when it comes to their chosen pursuit. One thing’s for sure: our physical risk-takers are definitely not part of the Village. (3934)\n",
    "\n",
    "Successful risk-takers are cool under pressure. They don’t try to be heroes, but they can execute when the chips are down. (3988)\n",
    "\n",
    "Successful risk-takers have courage. They’re insanely competitive and their attitude is: bring it on. (4013)\n",
    "\n",
    "Successful risk-takers have strategic empathy. They put themselves in their opponent’s shoes. (4035)\n",
    "\n",
    "Successful risk-takers are process oriented, not results oriented. They play the long game. (4067)\n",
    "\n",
    "Successful risk-takers take shots. They are explicitly aware of the risks they’re taking—and they’re comfortable with failure. (4098)\n",
    "\n",
    "Successful risk-takers take a raise-or-fold attitude toward life. They abhor mediocrity and they know when to quit. (4127)\n",
    "\n",
    "Successful risk-takers are prepared. They make good intuitive decisions because they’re well trained—not because they “wing it.” (4171)\n",
    "\n",
    "Successful risk-takers have selectively high attention to detail. They understand that attention is a scarce resource and think carefully about how to allocate it. (4193)\n",
    "\n",
    "Successful risk-takers are adaptable. They are good generalists, taking advantage of new opportunities and responding to new threats. (4230)\n",
    "\n",
    "Successful risk-takers are good estimators. They are Bayesians, comfortable quantifying their intuitions and working with incomplete information. (4252)\n",
    "\n",
    "Successful risk-takers try to stand out, not fit in. They have independence of mind and purpose. (4284)\n",
    "\n",
    "Successful risk-takers are conscientiously contrarian. They have theories about why and when the conventional wisdom is wrong. (4306)\n",
    "\n",
    "Successful risk-takers are not driven by money. They live on the edge because it’s their way of life. (4350)\n",
    "\n",
    "Is it true that (most) successful risk-takers are not driven by money? Is that in any way in opposition to the edge being a way of life? In the end, most people are in key senses not driven by money. The money is either the means to an end, or it is ‘the score.’ But money is still highly motivational, as that means to an end or as that score. Was SBF motivated by money, or not? What’s the difference?\n",
    "\n",
    "Of all these, I’d say the most underappreciated is being process oriented. Missing that will get you absolutely killed.\n",
    "\n",
    "Whereas, as we see when we get to Silicon Valley, being unaware of the risks or odds can kind of work out in some situations. So can, as we see with the astronaut, not being contrarian.\n",
    "\n",
    "So for example on a spacecraft:\n",
    "\n",
    "On the Blue Origin orbital spacecraft. Vescovo, who is also a former commander in the U.S. Navy Reserve, told me that the mentality required in exploration, the military, and investing is more similar than you might think. “It’s risk assessment and taking calculated risks,” he said. “And then trying to adapt to circumstances. I mean, you can’t be human and not engage in some degree of risk-taking on a day-to-day basis, I’m just taking it to a different level.” (3983)\n",
    "\n",
    "Or for a pilot, fictional bad example edition:\n",
    "\n",
    "What most annoyed Vescovo about Top Gun: Maverick was Tom Cruise’s insistence that you should just trust your gut and improvise your way out of a hairy situation. “The best military operations are the ones that are very boring, where things go exactly according to plan. No one’s ever put in any danger,” he said. “You want to minimize the risks. And so yeah, Top Gun, it looked great on film. But that is not how you would try and take out that target.” (4172)\n",
    "\n",
    "I haven’t seen Top Gun: Maverick, but you don’t have to in order to understand.\n",
    "\n",
    "So is there never a place for trusting your gut? That’s not quite what Vescovo is saying. Rather, it’s that the more you train, the better your instincts will be. “When something is for real and is an emergency, how many times have you heard people say, ‘Oh, you know, the training kicked in.’ ” Training, ironically, is often the best preparation to handle the situations that you don’t train for. (4180)\n",
    "\n",
    "The problem with Top Gun: Maverick wasn’t with Maverick—his instincts probably were pretty good—but that he was imploring other pilots to trust their gut and be heroes when they didn’t have the same experience base. (4191)\n",
    "\n",
    "Yep. The way you trust your gut and improvise well is to be Crazy Prepared. Then you have a gut worth trusting. You can’t give that advice to people before they’re ready, and people often do it and cause a lot of damage or failure.\n",
    "\n",
    "“The best players will study solvers so much that their fundamentals become automatic,” he wrote—the complex System 2 solutions that computers come up with make their way into your instinctual System 1 with enough practice. That frees up mental bandwidth for when you do face a hairy situation, or a great opportunity. (4187)\n",
    "\n",
    "That’s how the true masters do it. The less you have to think about the basics, the more they’re automatic, the more you can improve other stuff.\n",
    "\n",
    "Exactly in the Magic competitions where I was Crazy Prepared I was able to figure out things under the lights I’d never considered in practice – for example in my victory in Tokyo, I’d not once named Nightscape Familiar on a Meddling Mage in all of practice, or named Green for Voice of All without a Crimson Acolyte against Red/Green, or even considered cutting Fact or Fiction while sideboarding. Winning required, as it played out, that I figure out to do all three.\n",
    "\n",
    "Who would take the risks without the pressure?\n",
    "\n",
    "Karikó found it more in the United States than in Communist-era Hungary. “If I would stay in Hungary,” she told me,[*6] “can you imagine I would go and sleep in the office?” In the United States, she found “the pressure is on in different things, so that is why it’s great.” (4029)\n",
    "\n",
    "For me the answer is that you can sort of backdoor into situations where the risks are so overwhelmingly +EV that you have no choice but to take them, and you have the security of knowing you have a safety net if you need one – I was never worried I would end up on the street or anything, I could always get a ‘real job’ (as I did at Jane Street), learn to code (well enough to earn money doing it) or even play poker.\n",
    "\n",
    "Aside on Covid\n",
    "In so many ways, our civilization handled Covid rather badly. Nate identifies correctly one of the two core mistakes, which was that it was a raise-or-fold situation, and we called, trying to muddle through without a plan.\n",
    "\n",
    "I’d argue, for instance, that the world might have been better off if it treated the COVID-19 pandemic as a raise-or-fold situation. (4148)\n",
    "\n",
    "The few countries like New Zealand and Sweden that pursued more coherent strategies—essentially, New Zealand raised and Sweden folded—did better than the many that muddled through with a compromise approach. (4150)\n",
    "\n",
    "This is essentially correct for the pre-vaccine period. Raising and folding were both reasonable options. The strategy we chose was neither, and we executed it badly, aside from (by our standards) Operation Warp Speed.\n",
    "\n",
    "The other core mistake was botched execution at every step. That’s another story.\n",
    "\n",
    "What is a Contrarian?\n",
    "What I think of as ‘contrarian’ Nate calls ‘independent’ here?\n",
    "\n",
    "There is an oft-neglected distinction between independence and contrarianism. If I pick vanilla and you pick chocolate because you like chocolate better, you’re being independent. If you pick chocolate because I picked vanilla, you’re being contrarian.\n",
    "\n",
    "Most people are pretty damned conformist—humans are social animals—and Riverians are sometimes accused of being contrarian when they’re just being independent. If I do the conventional thing 99 percent of the time and you do it 85 percent of the time, you’ll seem rebellious by comparison, but you’re still mostly going with the flow. (4307)\n",
    "\n",
    "This is in opposition to Nate saying ‘successful risk-takers are conscientiously contrarian.’ Successful risk-takers are being, by this terminology, independent.\n",
    "\n",
    "On the (OF COURSE!) contrary, I think of this very differently. Being ‘a contrarian’ means exactly what Nate calls independent here: Being willing to believe what seems true, and do what you prefer, and say it out loud, exactly because you think it is better.\n",
    "\n",
    "Most people are, most of the time, rather unwilling to do this. Even when they disagree or do something different, they are still following the script.\n",
    "\n",
    "Yes, they will sometimes pick chocolate over vanilla despite you picking vanilla. At other times, they will pick chocolate over vanilla in part because you picked vanilla… because it is standard to not order the same thing at the same time, so also picking literal vanilla would actually in many cases be contrary and independent. But they’ll still look for chocolate, not the weird sounding flavor they actually like and want, if they can make it work – which is similar to how those pushed out of The Village end up in The Wilderness (or Vortex), rather than in The River or doing some secret third thing (I think on reflection the secret thing is kind of Always Third, even if it’s not?).\n",
    "\n",
    "Someone who actually does unconventional things 15% of the time is a world-class rogue actor. You may think that someone (let’s say Elon Musk) is doing tons of unconventional stuff and is totally out in space, but when you add it all up he’s still on script something like 99% of the time. The difference is 99% versus 99.9%, or 99.99%.\n",
    "\n",
    "Which is smart. The script isn’t stupid, you shouldn’t go around breaking it to break it, and if you broke it 15% of the time you would, as they say, Find Out. When we say 85%, we mean that 15% of the time Elon is doing some aspect of the thing differently.\n",
    "\n",
    "After all, when you order the Cinnamon Toast ice cream, it’s still ice cream, and you are probably still putting it in a cup or cone and eating it, and so on.\n",
    "\n",
    "What Nate Silver is calling ‘contrarian’ here is what I’d call ‘oppositional.’ It is the thing where Your Political Party says ‘I think apple pie is good’ and Their Political Party says ‘well then I suppose apple pie is bad.’\n",
    "\n",
    "Prediction Market Smackdown\n",
    "I’m going to finish the introduction with Nate’s discussion of prediction markets.\n",
    "\n",
    "Nate Silver, now advisor to Polymarket, is definitely a prediction market fan.\n",
    "\n",
    "He still has reservations, because he has seen their work.\n",
    "\n",
    "My views are mostly sympathetic, but not without some reservations. That’s in part because of some scar tissue from too many arguments I’ve had on the internet about the accuracy of prediction markets versus FiveThirtyEight forecasts. The FiveThirtyEight forecasts have routinely been better—I know that’s what you were expecting me to say, but it’s true—something that’s not supposed to happen if the markets are efficient.\n",
    "\n",
    "Then again, maybe this doesn’t tell us that much. Elections are quite literally the Super Bowl of prediction markets—there’s so much dumb money out there (lots of people who have very strong opinions about politics) that there isn’t necessarily enough smart money to offset it. (6736)\n",
    "\n",
    "It makes sense that presidential elections are a place where prediction markets will be great for generating liquidity, and great for measuring how much various changes impact probabilities, but exhibit a strong bias, and sometimes be pretty far off.\n",
    "\n",
    "We certainly saw that in 2020, and the markets in 2008 and 2012 also had major issues.\n",
    "\n",
    "My guess is that Polymarket is biased in favor of Trump, because those trading in a crypto prediction market are going to have that bias, and because a lot of traders realize that if Harris wins they have a good chance of being able to buy Harris at 90%, or at least 95%, similar to what happened in 2020. That should in turn bias the odds now.\n",
    "\n",
    "There should still be plenty of reasons to keep this in check, so the market won’t be too far off. And changes over time should mostly reflect ground truth. PredictIt has Harris up 55-45 while I write this while Polymarket is 50-50, and Metaculus also has Harris at 55-45. That’s a substantial difference, but it sounds bigger than it is. Right now (the evening of 9/11/24) I think that PredictIt is right, but who wins won’t settle that question.\n",
    "\n",
    "One underrecognized pattern is that odds often do tend to stick at exactly even far more often than they should. So often they are selling dollars for fifty cents. There are a lot of people who are willing to bet at 50% odds, but no higher, and often one of them is willing to go big, so things get stuck there.\n",
    "\n",
    "But the bigger concern I have, ironically enough, is that prediction markets may become less reliable if people trust them too much. (6746)\n",
    "\n",
    "The danger is that they are trusted too much relative to their liquidity. In absolute terms, it would be fine to go all the way to Robin Hanson’s Futarchy, where prediction markets determine government decisions. But to do that, you need there to be enough liquidity for when people have biases, lose their minds or attempt manipulations.\n",
    "\n",
    "Checkpoint One\n",
    "We’ll wrap up there for today, and tomorrow resume with the wide world of gambling.\n",
    "\n",
    "This whole post you keep referencing \"the Hegelian dialectic\" as some sort of thing central to the Village, without ever stating what you think it is; could you elaborate?\n",
    "\n",
    "I'm not an expert in this area by any means but I do not have the impression that the Village has even a plurality of Hegelians, let alone being centered around them, and the way you talk about the Hegelian dialectic does not seem to be remotely the same usage that you see with actual Hegelians. Honestly I expect actual Hegelians to be rather less upset about the River's differences from them, because, to my understanding, Hegel's whole thing is he believes in a sort of teleology of history that guarantees the victory of human freedom?\n",
    "\n",
    "I noticed the same thing -- even Scott Alexander dropped a reference to it without explaining it. Anyway, here what I came up with:-\n",
    "\n",
    "(That's me done for another two days)\n",
    "\n",
    "However I also do frequently spend more time on close decisions. I think this can be good praxis. It is wasteful in the moment, but going into detail on close decisions is a great way to learn how to make better decisions. So in any decision where it would be great to improve your algorithm, if it is very close, you might want to overthink things for that reason.\n",
    "\n",
    "In my experience, the more effective way to learn from close decisions is to just pick one alternative and then study the outcome and overthink the choice, rather than deliberate harder before choosing. This is related to what Cedric Chin describes in Action Produces Information: by going faster through close decisions, we both have more information about the consequences revealed to us, and we can run more experiments in parallel.\n",
    "\n",
    "That said, I am very hardcore about coinflipping even not-so-close decisions, and made a tool for it.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d782530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "from supplementary import create_dataloader_v1\n",
    "\n",
    "\n",
    "# Train/validation ratio\n",
    "train_ratio = 0.90\n",
    "split_idx = int(train_ratio * len(text))\n",
    "train_data = text[:split_idx]\n",
    "val_data = text[split_idx:]\n",
    "\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_loader = create_dataloader_v1(\n",
    "    train_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=True,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "val_loader = create_dataloader_v1(\n",
    "    val_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=False,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8fc75f73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000000): Train loss 8.516, Val loss 8.726\n",
      "Ep 1 (Step 000005): Train loss 7.341, Val loss 7.476\n",
      "Ep 1 (Step 000010): Train loss 6.577, Val loss 7.107\n",
      "Ep 1 (Step 000015): Train loss 6.027, Val loss 6.863\n",
      "Ep 1 (Step 000020): Train loss 6.012, Val loss 6.792\n",
      "What do you think about A Brain for Innovation?                                                  \n",
      "Ep 2 (Step 000025): Train loss 5.399, Val loss 6.783\n",
      "Ep 2 (Step 000030): Train loss 5.430, Val loss 6.867\n",
      "Ep 2 (Step 000035): Train loss 5.062, Val loss 6.831\n",
      "Ep 2 (Step 000040): Train loss 4.853, Val loss 6.762\n",
      "What do you think about A Brain for Innovation?                                                  \n",
      "Ep 3 (Step 000045): Train loss 4.256, Val loss 6.781\n",
      "Ep 3 (Step 000050): Train loss 4.399, Val loss 6.846\n",
      "Ep 3 (Step 000055): Train loss 3.804, Val loss 6.851\n",
      "Ep 3 (Step 000060): Train loss 3.479, Val loss 6.881\n",
      "Ep 3 (Step 000065): Train loss 3.599, Val loss 6.851\n",
      "What do you think about A Brain for Innovation?            Successful risk-takers are The River Nature and the Village, a lot, the Village, you have to The Village, which I’s a lot of the book, but\n",
      "Ep 4 (Step 000070): Train loss 3.154, Val loss 6.805\n",
      "Ep 4 (Step 000075): Train loss 3.043, Val loss 6.881\n",
      "Ep 4 (Step 000080): Train loss 2.667, Val loss 6.945\n",
      "Ep 4 (Step 000085): Train loss 2.740, Val loss 6.935\n",
      "What do you think about A Brain for Innovation?    I’takers are the Village, and other book—and that they’takers are the Village.       I asked. They’takers are good Riverians are not what\n",
      "Ep 5 (Step 000090): Train loss 2.243, Val loss 6.993\n",
      "Ep 5 (Step 000095): Train loss 2.172, Val loss 7.035\n",
      "Ep 5 (Step 000100): Train loss 2.087, Val loss 7.065\n",
      "Ep 5 (Step 000105): Train loss 1.648, Val loss 7.116\n",
      "What do you think about A Brain for Innovation?  The Village, but most also do not a big problem. The River, and I would be that it’t think.  The Village or even The Village, and the River Nature as a when we get to The River\n",
      "Ep 6 (Step 000110): Train loss 1.541, Val loss 7.130\n",
      "Ep 6 (Step 000115): Train loss 1.290, Val loss 7.177\n",
      "Ep 6 (Step 000120): Train loss 1.284, Val loss 7.248\n",
      "Ep 6 (Step 000125): Train loss 1.241, Val loss 7.270\n",
      "Ep 6 (Step 000130): Train loss 1.036, Val loss 7.385\n",
      "What do you think about A Brain for Innovation? What’s the difference?  Of all these, you’d say the most under the Riverians are being contrarian. Successful risk-takers are good estimators. They are Bayesians, you have to\n",
      "Ep 7 (Step 000135): Train loss 0.760, Val loss 7.460\n",
      "Ep 7 (Step 000140): Train loss 0.691, Val loss 7.427\n",
      "Ep 7 (Step 000145): Train loss 0.740, Val loss 7.475\n",
      "Ep 7 (Step 000150): Train loss 0.529, Val loss 7.568\n",
      "What do you think about A Brain for Innovation? What’t have a flight.   In Nate’t have the same experience base. “wing mirror image of The Village. No longer. They’s ever put in any danger, and Riverians are sometimes accused\n",
      "Ep 8 (Step 000155): Train loss 0.395, Val loss 7.601\n",
      "Ep 8 (Step 000160): Train loss 0.384, Val loss 7.662\n",
      "Ep 8 (Step 000165): Train loss 0.363, Val loss 7.703\n",
      "Ep 8 (Step 000170): Train loss 0.297, Val loss 7.834\n",
      "Ep 8 (Step 000175): Train loss 0.234, Val loss 7.768\n",
      "What do you think about A Brain for Innovation? What’s the spirit is largely on point, but that on many details Nate Silver here buys a bit too much into the River had a rough few years.  But here’s getting his revenge on Denton for outing Thiel.\n",
      "Ep 9 (Step 000180): Train loss 0.276, Val loss 7.811\n",
      "Ep 9 (Step 000185): Train loss 0.226, Val loss 7.813\n",
      "Ep 9 (Step 000190): Train loss 0.144, Val loss 7.913\n",
      "Ep 9 (Step 000195): Train loss 0.176, Val loss 7.846\n",
      "What do you think about A Brain for Innovation? What’s the difference?  Of all these, I’d say the most underappreciated is being process oriented. Missing that will get you absolutely killed.  Whereas, as we see when we get to Silicon Valley\n",
      "Ep 10 (Step 000200): Train loss 0.177, Val loss 8.012\n",
      "Ep 10 (Step 000205): Train loss 0.167, Val loss 7.976\n",
      "Ep 10 (Step 000210): Train loss 0.120, Val loss 7.995\n",
      "Ep 10 (Step 000215): Train loss 0.090, Val loss 8.011\n",
      "What do you think about A Brain for Innovation? What’s the difference?  Of all these, I’d say the most underappreciated is being process oriented. Missing that will get you absolutely killed.  Whereas, as we see when we get to Silicon Valley\n"
     ]
    }
   ],
   "source": [
    "from supplementary import calc_loss_batch, evaluate_model, generate_and_print_sample\n",
    "\n",
    "def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n",
    "                       eval_freq, eval_iter, start_context, tokenizer):\n",
    "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
    "    tokens_seen, global_step = 0, -1\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for input_batch, target_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            tokens_seen += input_batch.numel()\n",
    "            global_step += 1\n",
    "\n",
    "            # evaluate\n",
    "            if global_step % eval_freq == 0:\n",
    "                train_loss, val_loss = evaluate_model(model, train_loader, val_loader, device, eval_iter)\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                track_tokens_seen.append(tokens_seen)\n",
    "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
    "                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
    "        \n",
    "        generate_and_print_sample(model, tokenizer, device, start_context)\n",
    "    \n",
    "    return train_losses, val_losses, track_tokens_seen\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)\n",
    "\n",
    "train_losses, val_losses, track_tokens_seens = train_model_simple(model, train_loader, val_loader, optimizer, device,\n",
    "    num_epochs=10, eval_freq=5, eval_iter=5,\n",
    "    start_context=\"What do you think about A Brain for Innovation?\", tokenizer=tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46199597",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "11f349d5-35e4-4502-8b86-ab57b5ca2f0c",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "\n",
    "# Solution to Exercise 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f564c82a-49f7-46da-ad78-b9cb846eb5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_context = \"Every effort moves you\"\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(start_context, tokenizer).to(device),\n",
    "    max_new_tokens=10,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9d58e1-afba-44c7-9f82-7516adff359d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64b3b1f-c8d3-4755-a926-dc86eeae0ba0",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "\n",
    "# Solution to Exercise 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a998656c-3615-4673-a9f9-c8eefb6b6611",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Imports from a local file\n",
    "from supplementary import GPTModel\n",
    "\n",
    "\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.load_state_dict(torch.load(\"model.pth\", map_location=device))\n",
    "model.eval()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
